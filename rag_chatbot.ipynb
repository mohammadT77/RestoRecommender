{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WJdr4FOV3m1",
        "outputId": "119f9efd-5cf0-4670-f37c-29bc797e5e5e"
      },
      "outputs": [],
      "source": [
        "!pip install -q \\\n",
        "    transformers==4.31.0 \\\n",
        "    accelerate==0.21.0 \\\n",
        "    bitsandbytes==0.41.0 \\\n",
        "    sentence-transformers==2.2.2 \\\n",
        "    xformers==0.0.20 \\\n",
        "\n",
        "!pip install -q \\\n",
        "    langchain==0.1.0 \\\n",
        "    langchain-community==0.0.12 \\\n",
        "    langchainhub==0.1.14 \\\n",
        "    faiss-gpu \\\n",
        "    faiss-cpu\n",
        "\n",
        "!pip install -q pandas\n",
        "# !pip install -q colab-xterm\n",
        "!pip install -qU langchain-anthropic\n",
        "!pip install -q python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVZTYMYqhEyD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from langchain.prompts import (\n",
        "    PromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    ChatPromptTemplate,)\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.document_loaders.dataframe import DataFrameLoader\n",
        "from langchain.storage import LocalFileStore\n",
        "from langchain.embeddings import CacheBackedEmbeddings\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "\n",
        "\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import huggingface_hub as hf_hub\n",
        "import torch\n",
        "\n",
        "try:\n",
        "    from google.colab.userdata import get as getenv\n",
        "    print(\"Running in colab\")\n",
        "except ImportError:\n",
        "    from os import getenv\n",
        "    import dotenv\n",
        "    dotenv.load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    from torch import cuda\n",
        "    device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "except ImportError:\n",
        "    device = 'cpu'\n",
        "\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wj12nPpoijV",
        "outputId": "62d61f31-5786-4169-d678-2c2137fada1b"
      },
      "outputs": [],
      "source": [
        "HF_TOKEN = getenv('HF_TOKEN')\n",
        "assert HF_TOKEN, \"A valid HuggingFace token is required to be set as <HF_TOKEN>.\"\n",
        "hf_hub.login(HF_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ANTHROPIC_API_KEY = getenv('ANTHROPIC_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rzeDBNMoijW"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeXyY2YLWqcD"
      },
      "outputs": [],
      "source": [
        "# Dataset files\n",
        "PLACES_PATH = \"data/places.csv\"\n",
        "REVIEWS_PATH = \"data/reviews.csv\"\n",
        "\n",
        "## Models\n",
        "# LLM_MODEL format: <model_type>::<model_name>\n",
        "# \n",
        "# model types: ['hf', 'anthropic']\n",
        "# \n",
        "# Example:\n",
        "# hf::meta-llama/Llama-2-7b-hf\n",
        "# anthropic::claude-3-sonnet-20240229\n",
        "# \n",
        "LLM_MODEL = \"anthropic::claude-3-sonnet-20240229\"\n",
        "EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
        "\n",
        "# Embeddings\n",
        "EMBEDDINGS_CACHE_STORE=\"./cache/\"\n",
        "\n",
        "# Faiss\n",
        "FAISS_REVIEWS_PATH = \"faiss_index\"\n",
        "FAISS_INDEX_NAME = \"index\"\n",
        "FAISS_DISTANCE_STRATEGY='EUCLIDEAN_DISTANCE'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUr5hUrPoijY"
      },
      "source": [
        "## Load Dataset\n",
        "\n",
        "Here we are using 2 csv files containing places (restuarants, bars, ...) info and reviews for each of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aONHvzXAoijZ"
      },
      "outputs": [],
      "source": [
        "def get_documents(content_func=lambda row:row['review'],\n",
        "                  source_func=lambda row:row['place_id'],\n",
        "                  metadata_fields=[]):\n",
        "\n",
        "  # Load both data files\n",
        "  places_df = pd.read_csv(PLACES_PATH)\n",
        "  reviews_df = pd.read_csv(REVIEWS_PATH)\n",
        "\n",
        "  # merge them on 'place_id'\n",
        "  merged_df = pd.merge(places_df, reviews_df, on='place_id', how='inner')\n",
        "\n",
        "  # add page_content and source columns using their corresponing functions\n",
        "  merged_df['page_content'] = merged_df.apply(content_func, axis=1)\n",
        "  merged_df['source'] = merged_df.apply(source_func, axis=1)\n",
        "\n",
        "  # update metadata_fields with 'page_content', 'source'\n",
        "  metadata_fields = list(set(metadata_fields + ['page_content', 'source']))\n",
        "\n",
        "  loader = DataFrameLoader(merged_df[metadata_fields],page_content_column='page_content')\n",
        "  return loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5jUkJ8XoijZ"
      },
      "outputs": [],
      "source": [
        "def content_func(row) -> str:\n",
        "  content_fields = ['place_name', 'place_types', 'place_address', 'place_average_ratings', 'review']\n",
        "  return '\\n'.join(f\"{key}={row[key]}\" for key in content_fields)\n",
        "\n",
        "documents = get_documents(content_func)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WH4HR9emoijZ",
        "outputId": "fa47e9ba-3bdf-4c36-c553-300479119540"
      },
      "outputs": [],
      "source": [
        "documents[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLU4f9yqoija"
      },
      "source": [
        "## Load Embeddings model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgtePYxKoija"
      },
      "outputs": [],
      "source": [
        "def get_hf_embedding_model(embedding_model_name,\n",
        "                           cache_embeddings_store,\n",
        "                           device='cpu',\n",
        "                           normalize_embeddings=False,\n",
        "                           ):\n",
        "  model_kwargs = {'device': device}\n",
        "  encode_kwargs = {'normalize_embeddings': normalize_embeddings} # Set `True` for cosine similarity\n",
        "  embedding_model = HuggingFaceEmbeddings(\n",
        "      model_name=embedding_model_name,\n",
        "      model_kwargs=model_kwargs,\n",
        "      encode_kwargs=encode_kwargs\n",
        "      )\n",
        "  store = LocalFileStore(cache_embeddings_store)\n",
        "  embedding_model = CacheBackedEmbeddings.from_bytes_store(\n",
        "                    embedding_model, store)\n",
        "  return embedding_model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIsrrLWnoijg"
      },
      "outputs": [],
      "source": [
        "embedding_model = get_hf_embedding_model(EMBEDDING_MODEL_NAME,\n",
        "                                         EMBEDDINGS_CACHE_STORE,\n",
        "                                         device=device,\n",
        "                                         normalize_embeddings=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZU69PxDoijg"
      },
      "source": [
        "## Load FAISS (Vector Database)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClgYJkU5oijj"
      },
      "outputs": [],
      "source": [
        "def get_vector_database(documents, embedding_model):\n",
        "\n",
        "  vector_database = FAISS.from_documents(\n",
        "      documents, embedding_model,\n",
        "      distance_strategy= FAISS_DISTANCE_STRATEGY\n",
        "      )\n",
        "  return vector_database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZY6Ku6Eboijj"
      },
      "outputs": [],
      "source": [
        "vector_db = get_vector_database(documents, embedding_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_kv8O3zndtA"
      },
      "outputs": [],
      "source": [
        "## if you want to save the db and use the files to load it again later.\n",
        "vector_db.save_local(folder_path=FAISS_REVIEWS_PATH, index_name=FAISS_INDEX_NAME)\n",
        "vector_db = FAISS.load_local(folder_path=FAISS_REVIEWS_PATH,\n",
        "                             embeddings=embedding_model,\n",
        "                             index_name=FAISS_INDEX_NAME)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzaIRkhToijm"
      },
      "outputs": [],
      "source": [
        "docs = vector_db.similarity_search(\"which one is the best pizza restaurant in the city?\", k = 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9xMBNVuoijt",
        "outputId": "9fa4cbc2-084d-4f1f-e891-6e00f457e0fd"
      },
      "outputs": [],
      "source": [
        "docs[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rk1-sq8Voijw"
      },
      "source": [
        "### Load LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOGUVg7uw4hP"
      },
      "outputs": [],
      "source": [
        "def get_anthropic_api_llm(model_name):\n",
        "  llm = ChatAnthropic(model_name=model_name, anthropic_api_key=ANTHROPIC_API_KEY,)\n",
        "\n",
        "  return llm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgNq7ELyoijx"
      },
      "outputs": [],
      "source": [
        "def get_hf_llm(model_name):\n",
        "\n",
        "  bnb_config = BitsAndBytesConfig(\n",
        "      load_in_4bit=True,\n",
        "      bnb_4bit_use_double_quant=True,\n",
        "      bnb_4bit_quant_type=\"nf4\",\n",
        "      bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "  )\n",
        "  model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, )\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "  pipe = pipeline(\n",
        "      model=model,\n",
        "      tokenizer=tokenizer,\n",
        "      return_full_text=True,  # langchain expects the full text\n",
        "      task='text-generation',\n",
        "      # we pass model parameters here too\n",
        "      temperature=0.0001,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
        "      max_new_tokens=512,  # mex number of tokens to generate in the output\n",
        "      repetition_penalty=1.1  # without this output begins repeating\n",
        "  )\n",
        "\n",
        "  llm = HuggingFacePipeline(pipeline=pipe,)\n",
        "  return llm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grd0zxY9oijy"
      },
      "outputs": [],
      "source": [
        "model_type, _, model_name = LLM_MODEL.partition('::')\n",
        "\n",
        "# If model type is not set, use anthropic\n",
        "if model_name == \"\":\n",
        "    model_type = \"anthropic\"\n",
        "    model_name = model_type\n",
        "\n",
        "\n",
        "if model_type == \"anthropic\":\n",
        "    llm = get_anthropic_api_llm(model_name)\n",
        "else:\n",
        "    llm = get_hf_llm(model_name)\n",
        "    \n",
        "llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0ySJ4wyv78o",
        "outputId": "ca910516-7310-4ccb-8e57-c0bdfa3d5a85"
      },
      "outputs": [],
      "source": [
        "llm.invoke(\"Hi\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4SU2uzJoijz"
      },
      "source": [
        "## Create LangChain pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNPL6u4EkAW3"
      },
      "outputs": [],
      "source": [
        "review_template_str = \"\"\"\n",
        "Your job is to use Google Map restaurants and bars reviews to help people find best places to go for a meal or a drink.\n",
        "Use the following information and reviews to answer the questions.\n",
        "If you don't know an answer based on the context, say you don't know. Answer context:\n",
        "{context}\n",
        "\"\"\"\n",
        "## \"\"\"\n",
        "# If you don't know an answer based on the context, say you don't know, and\n",
        "# if the context is not about restaurants, then kindly tell them that  you can\n",
        "# only provide assistance and answer questions related to restaurants.\n",
        "##\"\"\"\n",
        "\n",
        "review_system_prompt = SystemMessagePromptTemplate(\n",
        "    prompt=PromptTemplate(\n",
        "        input_variables=[\"context\"], template=review_template_str\n",
        "    )\n",
        ")\n",
        "\n",
        "review_human_prompt = HumanMessagePromptTemplate(\n",
        "    prompt=PromptTemplate(input_variables=[\"question\"], template=\"{question}\")\n",
        ")\n",
        "messages = [review_system_prompt, review_human_prompt]\n",
        "\n",
        "review_prompt_template = ChatPromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"], messages=messages\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "reviews_retriever = vector_db.as_retriever(k=10,)\n",
        "\n",
        "review_chain = (\n",
        "    {\"context\": reviews_retriever, \"question\": RunnablePassthrough()}\n",
        "    | review_prompt_template\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32GruWzvuWnM",
        "outputId": "b53bc31b-0755-4fe6-99ce-5539fccb8b05"
      },
      "outputs": [],
      "source": [
        "# review_chain = (\n",
        "#     {\"context\": reviews_retriever, \"question\": RunnablePassthrough()}\n",
        "#     | review_prompt_template\n",
        "# )\n",
        "\n",
        "question = \"\"\"Where can I find delicious pizzas?\"\"\"\n",
        "print(review_chain.invoke(question))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzzRDTT6tTrt",
        "outputId": "2ea12f61-646e-45fb-839c-595749536e43"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"What are the pros and cons of the best pizza restaurant in the city?\"\"\"\n",
        "print(review_chain.invoke(question))\n",
        "\n",
        "# {\"context\": reviews_retriever, \"question\": RunnablePassthrough()}\n",
        "\n",
        "# review_prompt_template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1VUet58nE55"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"What are the pros and cons of the best pizza restaurant in the city?\"\"\"\n",
        "review_chain.invoke(question)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
