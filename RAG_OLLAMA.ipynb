{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNKGfzb_WbX4"
      },
      "source": [
        "## Installing packages and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3N9ZDVGxLyBc",
        "outputId": "f46deb27-c76c-47ec-b6cf-eb0f618c9932"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ollama\n",
            "  Downloading ollama-0.2.0-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting httpx<0.28.0,>=0.27.0 (from ollama)\n",
            "  Using cached httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting anyio (from httpx<0.28.0,>=0.27.0->ollama)\n",
            "  Using cached anyio-4.3.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: certifi in ./venv/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<0.28.0,>=0.27.0->ollama)\n",
            "  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: idna in ./venv/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.7)\n",
            "Collecting sniffio (from httpx<0.28.0,>=0.27.0->ollama)\n",
            "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama)\n",
            "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Downloading ollama-0.2.0-py3-none-any.whl (9.5 kB)\n",
            "Using cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "Using cached anyio-4.3.0-py3-none-any.whl (85 kB)\n",
            "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "Installing collected packages: sniffio, h11, httpcore, anyio, httpx, ollama\n",
            "Successfully installed anyio-4.3.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 ollama-0.2.0 sniffio-1.3.1\n",
            "Collecting langchain==0.1.0\n",
            "  Downloading langchain-0.1.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting langchain-community==0.0.12\n",
            "  Downloading langchain_community-0.0.12-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting langchainhub==0.1.14\n",
            "  Downloading langchainhub-0.1.14-py3-none-any.whl.metadata (478 bytes)\n",
            "Collecting PyYAML>=5.3 (from langchain==0.1.0)\n",
            "  Using cached PyYAML-6.0.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting SQLAlchemy<3,>=1.4 (from langchain==0.1.0)\n",
            "  Using cached SQLAlchemy-2.0.30-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain==0.1.0)\n",
            "  Using cached aiohttp-3.9.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.1.0)\n",
            "  Using cached dataclasses_json-0.6.6-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain==0.1.0)\n",
            "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-core<0.2,>=0.1.7 (from langchain==0.1.0)\n",
            "  Using cached langchain_core-0.1.52-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.77 (from langchain==0.1.0)\n",
            "  Downloading langsmith-0.0.92-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting numpy<2,>=1 (from langchain==0.1.0)\n",
            "  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting pydantic<3,>=1 (from langchain==0.1.0)\n",
            "  Using cached pydantic-2.7.1-py3-none-any.whl.metadata (107 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in ./venv/lib/python3.12/site-packages (from langchain==0.1.0) (2.31.0)\n",
            "Collecting tenacity<9.0.0,>=8.1.0 (from langchain==0.1.0)\n",
            "  Using cached tenacity-8.3.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub==0.1.14)\n",
            "  Downloading types_requests-2.32.0.20240523-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.0)\n",
            "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.0)\n",
            "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.0)\n",
            "  Using cached frozenlist-1.4.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.0)\n",
            "  Using cached multidict-6.0.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.0)\n",
            "  Using cached yarl-1.9.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.0)\n",
            "  Using cached marshmallow-3.21.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.0)\n",
            "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain==0.1.0)\n",
            "  Using cached jsonpointer-2.4-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "INFO: pip is looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-core<0.2,>=0.1.7 (from langchain==0.1.0)\n",
            "  Downloading langchain_core-0.1.51-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading langchain_core-0.1.50-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading langchain_core-0.1.49-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading langchain_core-0.1.48-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading langchain_core-0.1.47-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading langchain_core-0.1.46-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading langchain_core-0.1.45-py3-none-any.whl.metadata (5.9 kB)\n",
            "INFO: pip is still looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading langchain_core-0.1.44-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading langchain_core-0.1.43-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading langchain_core-0.1.42-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading langchain_core-0.1.41-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading langchain_core-0.1.40-py3-none-any.whl.metadata (5.9 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading langchain_core-0.1.39-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading langchain_core-0.1.38-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Downloading langchain_core-0.1.37-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Downloading langchain_core-0.1.36-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Downloading langchain_core-0.1.35-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Downloading langchain_core-0.1.34-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Downloading langchain_core-0.1.33-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in ./venv/lib/python3.12/site-packages (from langchain-core<0.2,>=0.1.7->langchain==0.1.0) (4.3.0)\n",
            "  Downloading langchain_core-0.1.32-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Downloading langchain_core-0.1.31-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Downloading langchain_core-0.1.30-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Downloading langchain_core-0.1.29-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Downloading langchain_core-0.1.28-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Downloading langchain_core-0.1.27-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Downloading langchain_core-0.1.26-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Downloading langchain_core-0.1.25-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Downloading langchain_core-0.1.24-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Downloading langchain_core-0.1.23-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.77 (from langchain==0.1.0)\n",
            "  Downloading langsmith-0.0.87-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2,>=0.1.7->langchain==0.1.0)\n",
            "  Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1->langchain==0.1.0)\n",
            "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.18.2 (from pydantic<3,>=1->langchain==0.1.0)\n",
            "  Using cached pydantic_core-2.18.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
            "Collecting typing-extensions>=4.6.1 (from pydantic<3,>=1->langchain==0.1.0)\n",
            "  Using cached typing_extensions-4.11.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests<3,>=2->langchain==0.1.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.12/site-packages (from requests<3,>=2->langchain==0.1.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests<3,>=2->langchain==0.1.0) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.12/site-packages (from requests<3,>=2->langchain==0.1.0) (2024.2.2)\n",
            "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain==0.1.0)\n",
            "  Using cached greenlet-3.0.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in ./venv/lib/python3.12/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.7->langchain==0.1.0) (1.3.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.0)\n",
            "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain-0.1.0-py3-none-any.whl (797 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.0/798.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.0.12-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
            "\u001b[?25hDownloading langchainhub-0.1.14-py3-none-any.whl (3.4 kB)\n",
            "Using cached aiohttp-3.9.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "Using cached dataclasses_json-0.6.6-py3-none-any.whl (28 kB)\n",
            "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langchain_core-0.1.23-py3-none-any.whl (241 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading langsmith-0.0.87-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "Using cached pydantic-2.7.1-py3-none-any.whl (409 kB)\n",
            "Using cached pydantic_core-2.18.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "Using cached PyYAML-6.0.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (724 kB)\n",
            "Using cached SQLAlchemy-2.0.30-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "Using cached tenacity-8.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading types_requests-2.32.0.20240523-py3-none-any.whl (15 kB)\n",
            "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
            "Using cached frozenlist-1.4.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (281 kB)\n",
            "Using cached greenlet-3.0.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (625 kB)\n",
            "Using cached jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Using cached marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
            "Using cached multidict-6.0.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
            "Using cached typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
            "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Using cached yarl-1.9.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (322 kB)\n",
            "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: typing-extensions, types-requests, tenacity, PyYAML, packaging, numpy, mypy-extensions, multidict, jsonpointer, greenlet, frozenlist, attrs, annotated-types, yarl, typing-inspect, SQLAlchemy, pydantic-core, marshmallow, langchainhub, jsonpatch, aiosignal, pydantic, dataclasses-json, aiohttp, langsmith, langchain-core, langchain-community, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed PyYAML-6.0.1 SQLAlchemy-2.0.30 aiohttp-3.9.5 aiosignal-1.3.1 annotated-types-0.7.0 attrs-23.2.0 dataclasses-json-0.6.6 frozenlist-1.4.1 greenlet-3.0.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.0 langchain-community-0.0.12 langchain-core-0.1.23 langchainhub-0.1.14 langsmith-0.0.87 marshmallow-3.21.2 multidict-6.0.5 mypy-extensions-1.0.0 numpy-1.26.4 packaging-23.2 pydantic-2.7.1 pydantic-core-2.18.2 tenacity-8.3.0 types-requests-2.32.0.20240523 typing-extensions-4.11.0 typing-inspect-0.9.0 yarl-1.9.4\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss-gpu (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss-gpu\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: numpy in ./venv/lib/python3.12/site-packages (from faiss-cpu) (1.26.4)\n",
            "Downloading faiss_cpu-1.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install ollama\n",
        "!python -m pip install langchain==0.1.0 langchain-community==0.0.12 langchainhub==0.1.14\n",
        "!pip install faiss-gpu\n",
        "!pip install faiss-cpu\n",
        "# !pip install langchain-core"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CMJTQUu-mvQB"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "from langchain_community.llms import Ollama\n",
        "\n",
        "from langchain.embeddings import CacheBackedEmbeddings\n",
        "from langchain.storage import LocalFileStore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ollama Client\n",
        "\n",
        "### Option 1: Run Ollama server locally\n",
        "**In this approach, you need resources to run the Ollama server locally. Better to run it on Google Colab.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: colab-xterm in ./venv/lib/python3.12/site-packages (0.2.0)\n",
            "Requirement already satisfied: ptyprocess~=0.7.0 in ./venv/lib/python3.12/site-packages (from colab-xterm) (0.7.0)\n",
            "Requirement already satisfied: tornado>5.1 in ./venv/lib/python3.12/site-packages (from colab-xterm) (6.4)\n",
            "The colabxterm extension is already loaded. To reload it, use:\n",
            "  %reload_ext colabxterm\n"
          ]
        }
      ],
      "source": [
        "# Install colab-xterm package and load the extension for running a terminal in Colab\n",
        "!pip install colab-xterm\n",
        "%load_ext colabxterm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Run the following cell and run these two commands in the Xterm:**\n",
        "\n",
        "```sh\n",
        "curl -fsSL https://ollama.com/install.sh | sh\n",
        "ollama serve & ollama pull llama2:7b\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%xterm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "OLLAMA_BASE_URL = \"http://localhost:11434\" \n",
        "OLLAMA_MODEL = \"llama2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRcUZjBOhbUT"
      },
      "source": [
        "### Option 2: Connect to a remote server\n",
        "This option gives you access to a remote Ollama server, thus you can run this code on your low-resources local machine.  \n",
        "**However, it would be slower due to the requests time to a remote server.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "OLLAMA_BASE_URL = \"https://63ab-34-125-22-255.ngrok-free.app\"\n",
        "OLLAMA_MODEL = \"llama3\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "REVIEWS_PATH=\"data/reviews.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "MWqQZk1HmXEV"
      },
      "outputs": [],
      "source": [
        "loader = CSVLoader(file_path=REVIEWS_PATH, source_column=\"text.text\",\n",
        "                    encoding = 'utf-8', metadata_columns= [\"place_id\", \"rating\"])\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get some insight and clean the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjVn8CrjmXCM",
        "outputId": "d2f7c83a-43e1-4c95-92f3-83f16fb3284f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "docs.page_content:\n",
            "name: places/ChIJpXSgrsDbfkcRzf_5kCMmrZI/reviews/ChdDSUhNMG9nS0VJQ0FnSUNUdnYyMnBBRRAB\n",
            "text.languageCode: en\n",
            "text.text: Huge \"eating palace\" which is lacking the fine accents of the italian food although the service was top. I would recommend for lunch, less for dinner\n",
            "\n",
            "docs.metadata:\n",
            "{'source': 'Huge \"eating palace\" which is lacking the fine accents of the italian food although the service was top. I would recommend for lunch, less for dinner', 'row': 0, 'place_id': 'ChIJpXSgrsDbfkcRzf_5kCMmrZI', 'rating': '4'}\n",
            "\n",
            "Maximum length of reviews in the dataset: 1700\n",
            "Maximum length of review < 2048, so we're good!\n"
          ]
        }
      ],
      "source": [
        "# @title Get some insight on a sample document\n",
        "def get_insight(docs):\n",
        "  anomalies = []\n",
        "  print(f\"docs.page_content:\\n{docs[0].page_content}\")\n",
        "  print(f\"\\ndocs.metadata:\\n{docs[0].metadata}\")\n",
        "  m = 0\n",
        "  for doc in docs:\n",
        "    review = doc.metadata[\"source\"]\n",
        "    l = len(review)\n",
        "    if l > m:\n",
        "        m = l\n",
        "    if l == 3427:\n",
        "      print(review)\n",
        "    if l >= 2040:\n",
        "      anomalies.append(doc.metadata[\"row\"])\n",
        "  print(f\"\\nMaximum length of reviews in the dataset: {m}\")\n",
        "  if m < 2048:\n",
        "    print(f\"Maximum length of review < 2048, so we're good!\")\n",
        "  else:\n",
        "    print(\"Maximum length of review >= 2028, so we're not good!\")\n",
        "  return anomalies\n",
        "\n",
        "anomalies = get_insight(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "gNwE1uh6tEf-"
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "list index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m documents\u001b[38;5;241m.\u001b[39mremove(documents[\u001b[43manomalies\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m])\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "documents.remove(documents[anomalies[-1]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmDnH_z7tgjH",
        "outputId": "b4308268-97bc-4cad-d61f-0e88c080cd83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "docs.page_content:\n",
            "name: places/ChIJpXSgrsDbfkcRzf_5kCMmrZI/reviews/ChdDSUhNMG9nS0VJQ0FnSUNUdnYyMnBBRRAB\n",
            "text.languageCode: en\n",
            "text.text: Huge \"eating palace\" which is lacking the fine accents of the italian food although the service was top. I would recommend for lunch, less for dinner\n",
            "\n",
            "docs.metadata:\n",
            "{'source': 'Huge \"eating palace\" which is lacking the fine accents of the italian food although the service was top. I would recommend for lunch, less for dinner', 'row': 0, 'place_id': 'ChIJpXSgrsDbfkcRzf_5kCMmrZI', 'rating': '4'}\n",
            "\n",
            "Maximum length of reviews in the dataset: 1700\n",
            "Maximum length of review < 2048, so we're good!\n"
          ]
        }
      ],
      "source": [
        "anomalies = get_insight(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9EmGLzEpUe9"
      },
      "source": [
        "Take a look at this [link](https://api.python.langchain.com/en/latest/embeddings/langchain_community.embeddings.ollama.OllamaEmbeddings.html#langchain-community-embeddings-ollama-ollamaembeddings) and this [link](https://api.python.langchain.com/en/latest/llms/langchain_community.llms.ollama.Ollama.html) for details of models parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "PVGFA2q4mhM2"
      },
      "outputs": [],
      "source": [
        "store = LocalFileStore(\"./cache/\")\n",
        "# can increase num_ctx up to 4,096 tokens!\n",
        "embedding_model = OllamaEmbeddings(model=OLLAMA_MODEL, num_ctx=2048, temperature=0, base_url=OLLAMA_BASE_URL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "hx2MxOObmhKh"
      },
      "outputs": [],
      "source": [
        "embedder = CacheBackedEmbeddings.from_bytes_store(\n",
        "    embedding_model, store, namespace=embedding_model.model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSLkFENwyFl0",
        "outputId": "7daa6eab-2aab-475b-9cfa-fa931220debe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([-0.5911625623703003,\n",
              "  -5.051942825317383,\n",
              "  0.16411244869232178,\n",
              "  -0.3182397782802582,\n",
              "  -0.049503616988658905],\n",
              " 4096)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# smaple embedding\n",
        "text = \"This is a test document.\"\n",
        "\n",
        "query_result = embedder.embed_query(text)\n",
        "query_result[:5], len(query_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibkAShS5yqha"
      },
      "source": [
        "API reference [FAISS](https://api.python.langchain.com/en/latest/vectorstores/langchain_community.vectorstores.faiss.FAISS.html#langchain_community.vectorstores.faiss.FAISS.similarity_search)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Nwl5m1U_maIf"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vector_db \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mdistance_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEUCLIDEAN_DISTANCE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# \"COSINE\"\u001b[39;00m\n",
            "File \u001b[0;32m~/RestoRecommender/venv/lib/python3.12/site-packages/langchain_core/vectorstores.py:508\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[0;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m texts \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    507\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 508\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/RestoRecommender/venv/lib/python3.12/site-packages/langchain_community/vectorstores/faiss.py:913\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[1;32m    888\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    894\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FAISS:\n\u001b[1;32m    895\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[1;32m    896\u001b[0m \n\u001b[1;32m    897\u001b[0m \u001b[38;5;124;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;124;03m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 913\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__from(\n\u001b[1;32m    915\u001b[0m         texts,\n\u001b[1;32m    916\u001b[0m         embeddings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    921\u001b[0m     )\n",
            "File \u001b[0;32m~/RestoRecommender/venv/lib/python3.12/site-packages/langchain/embeddings/cache.py:120\u001b[0m, in \u001b[0;36mCacheBackedEmbeddings.embed_documents\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    117\u001b[0m missing_texts \u001b[38;5;241m=\u001b[39m [texts[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m missing_indices]\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_texts:\n\u001b[0;32m--> 120\u001b[0m     missing_vectors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munderlying_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmissing_texts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocument_embedding_store\u001b[38;5;241m.\u001b[39mmset(\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(missing_texts, missing_vectors))\n\u001b[1;32m    123\u001b[0m     )\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index, updated_vector \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(missing_indices, missing_vectors):\n",
            "File \u001b[0;32m~/RestoRecommender/venv/lib/python3.12/site-packages/langchain_community/embeddings/ollama.py:204\u001b[0m, in \u001b[0;36mOllamaEmbeddings.embed_documents\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Embed documents using an Ollama deployed embedding model.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m    List of embeddings, one for each text.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    203\u001b[0m instruction_pairs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_instruction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[0;32m--> 204\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstruction_pairs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
            "File \u001b[0;32m~/RestoRecommender/venv/lib/python3.12/site-packages/langchain_community/embeddings/ollama.py:192\u001b[0m, in \u001b[0;36mOllamaEmbeddings._embed\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m     iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m--> 192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_emb_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m iter_]\n",
            "File \u001b[0;32m~/RestoRecommender/venv/lib/python3.12/site-packages/langchain_community/embeddings/ollama.py:157\u001b[0m, in \u001b[0;36mOllamaEmbeddings._process_emb_response\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    152\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    154\u001b[0m }\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/api/embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_default_params\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError raised by inference endpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/RestoRecommender/venv/lib/python3.12/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/RestoRecommender/venv/lib/python3.12/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/RestoRecommender/venv/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
            "File \u001b[0;32m~/RestoRecommender/venv/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
            "File \u001b[0;32m~/RestoRecommender/venv/lib/python3.12/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
            "File \u001b[0;32m~/RestoRecommender/venv/lib/python3.12/site-packages/urllib3/connectionpool.py:793\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    790\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    809\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/RestoRecommender/venv/lib/python3.12/site-packages/urllib3/connectionpool.py:537\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 537\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
            "File \u001b[0;32m~/RestoRecommender/venv/lib/python3.12/site-packages/urllib3/connection.py:466\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    465\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
            "File \u001b[0;32m/usr/lib/python3.12/http/client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1427\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1428\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1429\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
            "File \u001b[0;32m/usr/lib/python3.12/http/client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[0;32m/usr/lib/python3.12/http/client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/usr/lib/python3.12/socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[0;32m/usr/lib/python3.12/ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
            "File \u001b[0;32m/usr/lib/python3.12/ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "vector_db = FAISS.from_documents(documents, embedder,\n",
        "                                 distance_strategy=\"EUCLIDEAN_DISTANCE\") # \"COSINE\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUkJnFlymlTT"
      },
      "outputs": [],
      "source": [
        "docs = vector_db.similarity_search(\"which one is the best pizza restaurant in the city?\",\n",
        "                                      k = 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2kw-mzgwmWS",
        "outputId": "a9a95d49-d6b1-4903-8936-475adb9619d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content='name: places/ChIJ4aOff1rafkcRLpeSS2KAsPI/reviews/ChZDSUhNMG9nS0VJQ0FnSURqeU92Q0lBEAE\\ntext.languageCode: en\\ntext.text: Pizza was very and home made. Staff was fast, efficient and organized. I recommend this place!', metadata={'source': 'Pizza was very and home made. Staff was fast, efficient and organized. I recommend this place!', 'row': 607, 'place_id': 'ChIJ4aOff1rafkcRLpeSS2KAsPI', 'rating': '4'})"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BSoKN8mmlQ_"
      },
      "outputs": [],
      "source": [
        "docs = vector_db.similarity_search_with_score(\"which one is the best pizza restaurant in the city?\",\n",
        "                                      k = 5, )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OD22Jaew41A",
        "outputId": "94c1ff44-db78-4ab2-8f52-cc3abca63c78"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Document(page_content='name: places/ChIJ4aOff1rafkcRLpeSS2KAsPI/reviews/ChZDSUhNMG9nS0VJQ0FnSURqeU92Q0lBEAE\\ntext.languageCode: en\\ntext.text: Pizza was very and home made. Staff was fast, efficient and organized. I recommend this place!', metadata={'source': 'Pizza was very and home made. Staff was fast, efficient and organized. I recommend this place!', 'row': 607, 'place_id': 'ChIJ4aOff1rafkcRLpeSS2KAsPI', 'rating': '4'}),\n",
              " 8951.218)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlrdXjkIxBM6",
        "outputId": "3f9b84a2-6a24-48b2-a783-458109bd366c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Document(page_content=\"name: places/ChIJu9bZt0_afkcRsYoU1BCI1SI/reviews/ChZDSUhNMG9nS0VJQ0FnSUNUdUltVEl3EAE\\ntext.languageCode: en\\ntext.text: After a third visit to Padova and several fAfter three visits to Padova and several unsuccessful attempts, we finally managed to find a table in this wonderful osteria. The place has a charming, family-run feel with a lot of heart. It's cosy, quite and comfortable.\\r\\nThe food was brilliant, the wine was great, and the grappa was so tasty!\\r\\nWhen you travel to Padova, make sure to visit this place!\\r\\n(hard to find a free table, so make a reservation).\", metadata={'source': \"After a third visit to Padova and several fAfter three visits to Padova and several unsuccessful attempts, we finally managed to find a table in this wonderful osteria. The place has a charming, family-run feel with a lot of heart. It's cosy, quite and comfortable.\\r\\nThe food was brilliant, the wine was great, and the grappa was so tasty!\\r\\nWhen you travel to Padova, make sure to visit this place!\\r\\n(hard to find a free table, so make a reservation).\", 'row': 596, 'place_id': 'ChIJu9bZt0_afkcRsYoU1BCI1SI', 'rating': '5'}),\n",
              " 10440.775)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs[4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tb_Pk8vLNenz"
      },
      "outputs": [],
      "source": [
        "REVIEWS_FAISS_PATH = \"faiss_index\"\n",
        "FAISS_INDEX_NAME = \"index\"\n",
        "vector_db.save_local(folder_path=REVIEWS_FAISS_PATH, index_name=FAISS_INDEX_NAME)\n",
        "vector_db = FAISS.load_local(folder_path=REVIEWS_FAISS_PATH, embeddings=embedder, index_name=FAISS_INDEX_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tH64GamNFMVv"
      },
      "source": [
        "`db.as_retriever` has some cool options!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVHiV53KqGan"
      },
      "outputs": [],
      "source": [
        "# Retrieve more documents with higher diversity\n",
        "# Useful if your dataset has many similar documents\n",
        "vector_db.as_retriever(\n",
        "    search_type=\"mmr\",\n",
        "    search_kwargs={'k': 6, 'lambda_mult': 0.25}\n",
        ")\n",
        "\n",
        "# Fetch more documents for the MMR algorithm to consider\n",
        "# But only return the top 5\n",
        "vector_db.as_retriever(\n",
        "    search_type=\"mmr\",\n",
        "    search_kwargs={'k': 5, 'fetch_k': 50}\n",
        ")\n",
        "\n",
        "# Only retrieve documents that have a relevance score\n",
        "# Above a certain threshold\n",
        "vector_db.as_retriever(\n",
        "    search_type=\"similarity_score_threshold\",\n",
        "    search_kwargs={'score_threshold': 0.8}\n",
        ")\n",
        "\n",
        "# Only get the single most similar document from the dataset\n",
        "vector_db.as_retriever(search_kwargs={'k': 1})\n",
        "\n",
        "# Use a filter to only retrieve documents from a specific paper\n",
        "vector_db.as_retriever(\n",
        "    search_kwargs={'filter': {'paper_title':'GPT-4 Technical Report'}}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIfl80gglUSg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuMJnaAxA6bR"
      },
      "source": [
        "## Creating chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWj-Q-5mA5tL"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import (\n",
        "    PromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    ChatPromptTemplate,\n",
        ")\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "# from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "review_template_str = \"\"\"Your job is to use Google Map\n",
        "reviews to answer questions about their experience at a restaurant. Use\n",
        "the following context to answer questions. Be as detailed as possible, but\n",
        "don't make up any information that's not from the context. If you don't know\n",
        "an answer based on the context, say you don't know.\n",
        "context:\n",
        "{context}\n",
        "\"\"\"\n",
        "## \"\"\"\n",
        "# If you don't know an answer based on the context, say you don't know, and\n",
        "# if the context is not about restaurants, then kindly tell them that  you can\n",
        "# only provide assistance and answer questions related to restaurants.\n",
        "##\"\"\"\n",
        "\n",
        "review_system_prompt = SystemMessagePromptTemplate(\n",
        "    prompt=PromptTemplate(\n",
        "        input_variables=[\"context\"], template=review_template_str\n",
        "    )\n",
        ")\n",
        "\n",
        "review_human_prompt = HumanMessagePromptTemplate(\n",
        "    prompt=PromptTemplate(input_variables=[\"question\"], template=\"{question}\")\n",
        ")\n",
        "messages = [review_system_prompt, review_human_prompt]\n",
        "\n",
        "review_prompt_template = ChatPromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"], messages=messages\n",
        ")\n",
        "\n",
        "chat_model = Ollama(model=\"llama2\", temperature=0.)\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# REVIEWS_CHROMA_PATH = \"chroma_data/\"\n",
        "# reviews_vector_db = Chroma(\n",
        "#     persist_directory=REVIEWS_CHROMA_PATH,\n",
        "#     embedding_function=OpenAIEmbeddings(),\n",
        "# )\n",
        "reviews_vector_db = vector_db = FAISS.load_local(folder_path=REVIEWS_FAISS_PATH,\n",
        "                                         embeddings=embedder,\n",
        "                                         index_name=FAISS_INDEX_NAME)\n",
        "\n",
        "reviews_retriever = reviews_vector_db.as_retriever(k=10)\n",
        "\n",
        "review_chain = (\n",
        "    {\"context\": reviews_retriever, \"question\": RunnablePassthrough()}\n",
        "    | review_prompt_template\n",
        "    | chat_model\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "Nns-OuPTN-oy",
        "outputId": "13465a4e-59a4-41cf-c6b9-3fde79a4eacd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Based on the Google Map reviews provided, here are some pros and cons of the best pizza restaurant in the city:\\n\\nPros:\\n\\n1. Delicious food: The restaurant serves delicious pizzas that are well-liked by customers.\\n2. Good portions: The restaurant offers good portions of food, including their pizzas.\\n3. Organized staff: The staff is well-organized and efficient, ensuring a smooth dining experience.\\n4. Recommendation: Many reviewers have recommended this restaurant to others.\\n\\nCons:\\n\\n1. Crowded during lunchtime: The restaurant can get crowded during lunchtime, which may be inconvenient for some customers.\\n2. Limited seating: The restaurant has limited seating capacity, which can lead to long wait times for a table.\\n3. Difficulty finding a free table: It can be challenging to find a free table at the restaurant, especially during peak hours.\\n4. No reservation system: The restaurant does not have a reservation system in place, which can make it difficult for customers to secure a table without waiting.\\n\\nOverall, the best pizza restaurant in the city seems to have both positive and negative aspects. While the food is delicious and the staff is well-organized, the crowded atmosphere during lunchtime and limited seating capacity may be drawbacks for some customers.'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question = \"\"\"What are the pros and cons of the best pizza restaurant in the city?\"\"\"\n",
        "review_chain.invoke(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgOBHKynOgZT"
      },
      "outputs": [],
      "source": [
        "s = \"\"\"Based on the Google Map reviews provided, here are some pros and cons of the best pizza restaurant in the city:\n",
        "\n",
        "Pros:\n",
        "\n",
        "1. Delicious food: The restaurant serves delicious pizzas that are well-liked by customers.\n",
        "2. Good portions: The restaurant offers good portions of food, including their pizzas.\n",
        "3. Organized staff: The staff is well-organized and efficient, ensuring a smooth dining experience.\n",
        "4. Recommendation: Many reviewers have recommended this restaurant to others.\n",
        "\n",
        "Cons:\n",
        "\n",
        "1. Crowded during lunchtime: The restaurant can get crowded during lunchtime, which may be inconvenient for some customers.\n",
        "2. Limited seating: The restaurant has limited seating capacity, which can lead to long wait times for a table.\n",
        "3. Difficulty finding a free table: It can be challenging to find a free table at the restaurant, especially during peak hours.\n",
        "4. No reservation system: The restaurant does not have a reservation system in place, which can make it difficult for customers to secure a table without waiting.\n",
        "\n",
        "Overall, the best pizza restaurant in the city seems to have both positive and negative aspects. While the food is delicious and the staff is well-organized, the crowded atmosphere during lunchtime and limited seating capacity may be drawbacks for some customers.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLavLneWADc4"
      },
      "outputs": [],
      "source": [
        "# from langchain.agents import create_openai_functions_agent, Tool, AgentExecutor\n",
        "# from langchain import hub\n",
        "# from langchain_intro.tools import get_current_wait_time\n",
        "\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Reviews\",\n",
        "        func=review_chain.invoke,\n",
        "        description=\"\"\"Useful when you need to answer questions\n",
        "        about patient reviews or experiences at the hospital.\n",
        "        Not useful for answering questions about specific visit\n",
        "        details such as payer, billing, treatment, diagnosis,\n",
        "        chief complaint, hospital, or physician information.\n",
        "        Pass the entire question as input to the tool. For instance,\n",
        "        if the question is \"What do patients think about the triage system?\",\n",
        "        the input should be \"What do patients think about the triage system?\"\n",
        "        \"\"\",\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"Waits\",\n",
        "        func=get_current_wait_time,\n",
        "        description=\"\"\"Use when asked about current wait times\n",
        "        at a specific hospital. This tool can only get the current\n",
        "        wait time at a hospital and does not have any information about\n",
        "        aggregate or historical wait times. This tool returns wait times in\n",
        "        minutes. Do not pass the word \"hospital\" as input,\n",
        "        only the hospital name itself. For instance, if the question is\n",
        "        \"What is the wait time at hospital A?\", the input should be \"A\".\n",
        "        \"\"\",\n",
        "    ),\n",
        "]\n",
        "\n",
        "hospital_agent_prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
        "\n",
        "agent_chat_model = ChatOpenAI(\n",
        "    model=\"gpt-3.5-turbo-1106\",\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "hospital_agent = create_openai_functions_agent(\n",
        "    llm=agent_chat_model,\n",
        "    prompt=hospital_agent_prompt,\n",
        "    tools=tools,\n",
        ")\n",
        "\n",
        "hospital_agent_executor = AgentExecutor(\n",
        "    agent=hospital_agent,\n",
        "    tools=tools,\n",
        "    return_intermediate_steps=True,\n",
        "    verbose=True,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "nNKGfzb_WbX4"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
